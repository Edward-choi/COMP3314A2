{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training loss: 2.2925350300030414\n",
      "Epoch 1 training loss: 1.9770161892051128\n",
      "Epoch 2 training loss: 1.5350338945002444\n",
      "Epoch 3 training loss: 1.3177052794743194\n",
      "Epoch 4 training loss: 1.2140872888981915\n",
      "Epoch 5 training loss: 1.1422389785109808\n",
      "Epoch 6 training loss: 1.094171814827014\n",
      "Epoch 7 training loss: 1.0550559014399676\n",
      "Epoch 8 training loss: 1.0246667034590422\n",
      "Epoch 9 training loss: 1.0029496096853\n",
      "Epoch 10 training loss: 0.9839831478814326\n",
      "Epoch 11 training loss: 0.9534223621079663\n",
      "Epoch 12 training loss: 0.9402108055823393\n",
      "Epoch 13 training loss: 0.9213683487001513\n",
      "Epoch 14 training loss: 0.9130349113488756\n",
      "Epoch 15 training loss: 0.8976701567930453\n",
      "Epoch 16 training loss: 0.8950115882003231\n",
      "Epoch 17 training loss: 0.8777693810620542\n",
      "Epoch 18 training loss: 0.8697302394838475\n",
      "Epoch 19 training loss: 0.8593691056852402\n",
      "Epoch 20 training loss: 0.8506905125148261\n",
      "Epoch 21 training loss: 0.8401846659463098\n",
      "Epoch 22 training loss: 0.8326722415906789\n",
      "Epoch 23 training loss: 0.8171641783419449\n",
      "Epoch 24 training loss: 0.8242563006084865\n",
      "Epoch 25 training loss: 0.8084936512431611\n",
      "Epoch 26 training loss: 0.7975020909995667\n",
      "Epoch 27 training loss: 0.7932329705274944\n",
      "Epoch 28 training loss: 0.7932656125854581\n",
      "Epoch 29 training loss: 0.7838511339255742\n",
      "\n",
      "Training Time = 29.449009768168132  minutes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import os\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 12, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(12, 20, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(3920, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_dir = 'data/' # path of the data folder, can be edited\n",
    "\n",
    "trainset = datasets.ImageFolder(os.path.join(data_dir, 'train'), data_transforms['train'])\n",
    "\n",
    "testset = datasets.ImageFolder(os.path.join(data_dir, 'test'), data_transforms['test'])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "model_ft = Net() \n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_test(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    time0 = time.time()\n",
    "    for e in range(num_epochs):\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "\n",
    "            images = images.view(images.size(0), 3, 32, 32)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(images)\n",
    "            \n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            print(\"Epoch \" + str(e) + \" training loss: \" + str(running_loss/len(trainloader)))\n",
    "            \n",
    "    print(\"\\nTraining Time =\" ,(time.time()-time0)/60, \" minutes\")\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1)\n",
    "\n",
    "train_test(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft, 'data/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0    Number Of Images Tested = 500\n",
      "           Correct count (0) = 407\n",
      "           Model Accuracy = 0.814\n",
      "Class 1    Number Of Images Tested = 500\n",
      "           Correct count (1) = 391\n",
      "           Model Accuracy = 0.782\n",
      "Class 2    Number Of Images Tested = 500\n",
      "           Correct count (2) = 371\n",
      "           Model Accuracy = 0.742\n",
      "Class 3    Number Of Images Tested = 500\n",
      "           Correct count (3) = 261\n",
      "           Model Accuracy = 0.522\n",
      "Class 4    Number Of Images Tested = 500\n",
      "           Correct count (4) = 414\n",
      "           Model Accuracy = 0.828\n",
      "Class 5    Number Of Images Tested = 500\n",
      "           Correct count (5) = 359\n",
      "           Model Accuracy = 0.718\n",
      "Class 6    Number Of Images Tested = 500\n",
      "           Correct count (6) = 331\n",
      "           Model Accuracy = 0.662\n",
      "Class 7    Number Of Images Tested = 500\n",
      "           Correct count (7) = 429\n",
      "           Model Accuracy = 0.858\n",
      "Class 8    Number Of Images Tested = 500\n",
      "           Correct count (8) = 326\n",
      "           Model Accuracy = 0.652\n",
      "Class 9    Number Of Images Tested = 500\n",
      "           Correct count (9) = 358\n",
      "           Model Accuracy = 0.716\n"
     ]
    }
   ],
   "source": [
    "correct, total = 0, 0\n",
    "for l in range(0, 10):\n",
    "    Ccount, Acount = 0, 0\n",
    "    for images,labels in testloader:\n",
    "        for i in range(len(labels)):\n",
    "            img = images[i].view(1, 3, 32, 32)\n",
    "            with torch.no_grad():\n",
    "                logps = model_ft(img)\n",
    "\n",
    "\n",
    "            ps = torch.exp(logps)\n",
    "            probab = list(ps.numpy()[0])\n",
    "            pred_label = probab.index(max(probab))\n",
    "            true_label = labels.numpy()[i]\n",
    "            if(true_label == l):\n",
    "                Acount += 1\n",
    "                total += 1\n",
    "                if(true_label == pred_label):\n",
    "                    Ccount += 1\n",
    "                    correct += 1\n",
    "    print(\"Class \" + str(l) + \"    Number Of Images Tested = \" + str(Acount))\n",
    "    print(\"           Correct count (\" + str(l) + \") = \" + str(Ccount))\n",
    "    print(\"           Model Accuracy =\", (Ccount/Acount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7294"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct/5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), data_dir + 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=3920, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model = Net()\n",
    "Model.load_state_dict(torch.load('data/model'))\n",
    "Model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft, data_dir + 'Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=3920, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loadmodel = torch.load('data/Model')\n",
    "Loadmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
